---
title: "STAT 2450 Assignment 7 (50 points)"
author: "Your name here"
date: 'Banner:  B00783546'
output:
  html_document: default
  pdf_document: default
  word_document: default
---


# Problem 1: Surviving the Titanic (32 points)

Load the librairies

```{r}
if(!require(Hmisc)) install.packages("Hmisc",repos = "http://cran.us.r-project.org")
#library(Hmisc)
library("caret")
library("rpart")
library("tree")
library("e1071")
library(ggplot2) 
library(randomForest)
```

Load the data

```{r}
mytrain = read.csv("https://mathstat.dal.ca/~fullsack/DATA/titanictrain.csv")
mytest = read.csv("https://mathstat.dal.ca/~fullsack/DATA/titanictest.csv")
mytitanic = rbind(mytest,mytrain)
nrec=nrow(mytitanic)
```

You will be using the column 'Survived' as the outcome in our models. This should be treated as a factor.
All other columns are admissible as predictors of this outcome.

HINT-1: you can use the following template to split the data into folds, e.g. for cross-validation.

# Randomly shuffle your data
yourData<-yourData[sample(nrow(yourData)),]

# Create 10 pre-folds of equal size
myfolds <- cut(seq(1,nrow(yourData)),breaks=10,labels=FALSE)

# use these pre-folds for cross-validation

for(i in 1:10){ # loop over each of 10 folds
    # recover the indexes of fold i  and define the indexes of the test set 
    testIndexes <- which(myfolds==i,arr.ind=TRUE)
	# define yout test  for this fold
    testData <- yourData[testIndexes, ]
	# define your training set for this fold as the complement
    trainData <- yourData[-testIndexes, ]
    #  ....
}


HINT-2: Use the following template to split data into a train and a test set of roughly the same size

set.seed(44182) # or use the recommended seed
trainindex=sample(1:nrec,nrec/2,replace=F)
mytrain=mydata[trainindex,] # training set
mytest=mydata[-trainindex,] # testing set =  complementary subset of mydata


1. Define a 5 pre-folds of equal size of 'mytitanic' in a variable called 'myfolds'

(2 points)

```{r}
set.seed(2255)
# shuffle 
#
# Create 5 folds of equal size
# myfolds ...
myData<-mytitanic[sample(nrow(mytitanic)),]
myfolds<-cut(seq(1,nrow(mytitanic)),breaks=5,labels=FALSE)
```

2. Use pre-fold number 3 to define a testing and a training  set named 'mytest' and 'mytrain'

(2 points)

```{r}
i=3 # fold number to use
mytest=NULL
mytrain=NULL
for(j in 1:3){ # loop over each of 3 folds
    # recover the indexes of fold i  and define the indexes of the test set 
    testIndexes <- which(myfolds==i,arr.ind=TRUE)
	# define yout test  for this fold
    mytest<- mytitanic[testIndexes, ]
	# define your training set for this fold as the complement
    mytrain <- mytitanic[-testIndexes, ]
   
}
```

3. Fit a Random Forest model to the 'mytrain' dataset. Use the column 'Survived' as a factor outcome. Require importance to be true
and set the random seed to 523. (This is the 'trained model'). (2 points)


```{r}
# Fitting Random Forest Classification to the training set 'mytrain'
set.seed(523) # or use the recommended seed

randomf<-randomForest(as.factor(Survived)~.,data=mytrain,importance=T)
```

4. Plot the trained model results.

   Has the OOB error rate roughly equilibrated with 50 trees?
   
   Has the OOB error rate roughly equilibrated with 500 trees?

   What is the stationary value of the OOB error rate?  

   Which of death or survival has the smallest prediction error? 
   (4 points)
  

```{r}
plot(randomf)
#
```


5. Calculate the predictions on 'mytest', the misclassification error and the prediction accuracy. (2 points)


```{r}
# Predicting survival on mytest
randomf.pred <- predict(randomf,newdata=mytest[,-which(names(mytest)=="Survived")])

table(randomf.pred,mytest$Survived)

mean(mytest$Survived!=randomf.pred)
mean(mytest$Survived==randomf.pred)
```
The misclassification error is 0.1797753 and the prediction accuracy is 0.8202247. Results depend on situations.

6. Print and plot the importance of predictors in the trained model. (2 points)

```{r}
importance(randomf)
```


```{r}
varImpPlot(randomf)
```


Now you are going to have a more direct look at predictors for the records in 'mytest'.

Tabulate the chances of survival by the column 'Title'. What do you conclude? (2 points)
Which other predictor would have given you the same information? (1 points)

Are the predictors independent? (1 points)


```{r}
table(mytest$Title,mytest$Survived)
```
Conclusion: Male death is composed of the majority.

```{r}
table(mytest$Sex,mytest$Survived)
```
Sex predictor gives me the same information.


What is the median fare of passengers ? (1 points)
Hint: use the column 'Fare'

```{r}
median((mytest$Fare))

```

Tabulate the survival according to the binary variable mytest$Fare < 15    (2 points)

```{r}
table(mytest$Fare < 15,mytest$Survived)
```


```{r}
rm(mytrain,mytest)
#mytrain
```

7. Complete the code of the following function, which returns a vector of classification accuracies for $nrep$ random splits
into a training and testing sets of size half the number of records in the dataset 'mytitanic'.  (4 points)

```{r}
dotitan <- function(nrep,ntree,mtry){
set.seed(495)
acc = NULL
for(i in 1:nrep){

rm(mytrain,mytest)
nrec=nrow(mytitanic)

#define a train -test split as recommended  in the hints

# Fit a Random Forest Classification to the training set, using ntree trees and mtry predictors

# Predict the response on the testing set

# tabulate the prediction accuracy  ( Confusion matrix )

# compute the misclassification error 

# compute the classification accuracy

}

# return the classification accuracy

}

```

Run the function with 100 replicates, 500 trees per fit and 4 variables. (1 points)
Compute the mean accuracy and plot the histogram. Is the prediction performance of random forest highly variable?  (2 points)

```{r}
```

8. Once again, define a train-test split ('mytrain' and 'mytest') of 'mytitanic' of size ntrain=nrec/2, as recommended in the hints.
Take 332 for random seed.

```{r}

```

Run 50 independent fits of the random forest model, all using the SAME dataset mytrain.
Accumulate the accuracy of each fit in an array of size 50. Plot the histogram of this array. 
Do the different fits produce similar accuracies?  (4 points)

```{r}
```



# Problem 2 (18 points)

ISLR, chapter 8, problem 8. DO ALL PARTS. 

For part (a), use the following split into training and test sets.

```{r}
set.seed(44182)
library(ISLR)
library(randomForest)

attach(Carseats)
n=nrow(Carseats)
indices=sample(1:n,n/2,replace=F)
cstrain=Carseats[indices,]
cstest=Carseats[-indices,]
```

b)  5 points for plot and an estimate of test set MSE.
```{r}
library(tree)
train.tree=tree(Sales~.,data=cstrain)
plot(train.tree)
text(train.tree)
```
```{r}
testpredict=predict(train.tree, newdata = cstest)
testMSE = mean((cstest$Sales-testpredict)^2)
testMSE
```
The test MSE is 4.932839.
c)  3 points for reporting test set MSE for the pruned tree.
```{r}
salesCV = cv.tree(train.tree)
bestSize = salesCV$size[salesCV$dev == min(salesCV$dev)]

tree.pruned = prune.tree(train.tree, best = bestSize)
prunepredict = predict(tree.pruned, newdata = cstest)
pruneMSE = mean((cstest$Sales-prunepredict)^2)
pruneMSE
```
The test set MSE for pruned tree is 4.932839. It is equal with MSE training set data. Results depend on situation.
d)  3 points for output, and test set MSE.
```{r}
#bagging
sales.rf = randomForest(Sales~., data = cstrain, mtry = 10, importance = T)
importance(sales.rf)

trainpredict4d = predict(sales.rf, newdata = cstrain)
testpredict4d = predict(sales.rf, newdata = cstest)

testMSE4d=mean((cstrain$Sales-trainpredict4d)^2)
testMSE4d
```
The test MSE is 0.5044889.
e)  3 points. results will differ with different random sequences.
```{r}
sales.rf = randomForest(Sales~., data = cstrain, mtry = 2, importance = T)
importance(sales.rf)

trainpredict4e1 = predict(sales.rf, newdata = cstrain)
testpredict4e1 = predict(sales.rf, newdata = cstest)

testMSE4e1=mean((cstrain$Sales-trainpredict4e1)^2)
testMSE4e1
```
The test MSE for mtry=2 is 0.9026267.
f)  4 points for some sensible discussion of how the results are
seen to differ with a different value of mtry. Particular choice of mtry
is not important, as long as two different values were used.

```{r}
sales.rf = randomForest(Sales~., data = cstrain, mtry = 9, importance = T)
importance(sales.rf)

trainpredict4e2 = predict(sales.rf, newdata = cstrain)
testpredict4e2 = predict(sales.rf, newdata = cstest)

testMSE4e2=mean((cstrain$Sales-trainpredict4e2)^2)
testMSE4e2
```
From e the test MSE for mtry=2 is 0.9026267 and from f the test MSE for mtry=9 is 0.5134589. The value of mtry will affect the test MSE.



